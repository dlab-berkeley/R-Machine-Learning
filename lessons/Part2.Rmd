---
title: "R Machine Learning Pilot (Day 2)"
output: html_notebook
editor_options: 
  markdown: 
    wrap: 72
---

# Recap of Part 1

In Part 1 of this workshop, we attempted to predict the value of a
single, continuous outcome variable `hourly_wage` based on a range of
input features, including age, location, and industry. Today, we will
apply those principles to categorical variables in what's known as
'classification.'

# Some Workshop Notation

As a reminder, we'll use the same notation from Part 1:

üîî **Question**: A quick question to help you understand what's going
on.

ü•ä **Challenge**: Interactive exercise. We'll go through these in the
workshop!

‚ö†Ô∏è **Warning**: Heads-up about tricky stuff or common mistakes.

üí° **Tip**: How to do something a bit more efficiently or effectively.

üìù **Poll**: A zoom poll to help you learn.

üé¨ **Demo**: Showing off something more advanced so you know what you
can use R for in the future.

# Install Packages and Load Data

Before we dive into today's material, let's load our libraries
`tidymodels` and `tidyverse` libraries.

```{r install, include=FALSE}
# install.packages(c("tidyverse","tidymodels"))
library(tidymodels)
library(tidyverse)

# Prefer tidymodels functions in any case of name conflict
tidymodels::tidymodels_prefer() 

# set seed for reproducibility 
set.seed(123)
```

# Other Tasks in Supervised Machine Learning: Classification

Thus far, we've spent most of our time working through regression
problems. Let's switch to a new task: classification. In classification,
we aim to predict one of a group of values. For example, predicting a
qualitative response is considered classification because we assign the
observation to a category (or class).

Like regression, classification is also a supervised learning technique
because we have a set of labeled training data that we can use to build
a classifier. Now, however, we have access to different techniques
because the structure of our outcome variable is categorical.

üìù **Poll 1**: Identify which of the following are classification problems in machine learning. 

1.  An advertiser is interested in the relationship between age and the number of hours of YouTube consumed.

2.  A medical testing company conducts a procedure to determine whether a person has a cancer diagnosis.

3.  A researcher is interested in the effect of an education intervention on students' test scores.

4. A software engineer is designing an algorithm to detect whether an email is spam or not. 

5. A political scientist wants to classify Twitter posts as positive or negative. 

**Solution 1: (2), (4), and (5).**

# Example 1: Logistic Regression

Now, let's load our primary data set for today's workshop: `vote`. 

```{r}
vote <- read.csv("../data/vote.csv",row.names = NULL)

# visually inspect the data frame 
view(vote)
```


Machine learning practitioners often recommend logistic regression as a starting model when predicting a binary outcome or probability. For example, if we are estimating the relationship between mortality and income we can estimate the probability of mortality given a change in income.

We write this as $P[M|I]$ and the values will range between 0 and 1. We can make a prediction for any given value of income on the mortality outcome. Normally, we establish a threshold for prediction. For example, we might predict death where $P[M|i] > 0.5$.

**Question 1 : how do you determine from a probability ie logistic
regression to guess 1 or 0, ends up being coin flip**

Logistic regression is a generalized linear model where we model the probability function using the logistic function. Define the general function as $p(Y= 1|X)$. Then the model of interest is:

$$p(Y = 1|\textbf{X}) = \frac{e^{\textbf{X}\beta}}{1 + e^{\textbf{X}\beta}}$$
Here the bold X indicates a vector of features and the $\beta$
coefficient represents a vector of coefficients.

Here's how to fit classification problems in `tidymodels` using logistic regression.

Let's go ahead and perform the training and test splits for the `vote`
data set:

```{r}
# Perform splits
vote$voted <- as.factor(vote$voted)
class_split <- initial_split(vote, prop = 0.75)
class_train <- training(class_split)
class_test <- testing(class_split)
```

In `tidymodels`, creating a logistic regression follows the exact same procedure as a linear regression. This time, however, we will use `logistic_reg()` to initiate the function. Let's create the model:

```{r}
# Create model
logistic_model <- logistic_reg(mode = "classification")
```

Next, let's create a `recipe` for preprocessing:

```{r}
# Recipe for classifying defaults
class_recipe <- recipe(voted ~ ., data = class_train) %>%
  step_impute_median(all_numeric_predictors()) %>%
  step_impute_mode(all_nominal_predictors()) %>%
  step_dummy(all_nominal_predictors()) 
```

Now, we create our workflow, and fit the model:

```{r}
# Create workflow 
class_wflow <- workflow() %>%
  add_recipe(class_recipe) %>%
  add_model(logistic_model)

# Fit the workflow
class_fit <- class_wflow %>% 
  fit(data = class_train)
```

Finally, let's use augment to obtain the predictions, and take a look at
them:

```{r}
predictions <- augment(class_fit, new_data = class_test)
predictions
```

Notice how there are two columns for the predictions. What do these
values mean? What are they telling us?

To evaluate the model, we'll use the `accuracy` function again:

```{r}
accuracy(predictions, truth = voted, estimate = .pred_class, type="class")
```



Most of the defaults are "No". So, if we had naively just said nobody
defaulted, we would have obtained an accuracy of 97%! We did better than
this - 97.76% - but this shows us the importance of properly
benchmarking our data.

üí° **Tip**: While we used logistic regression to predict a binary
outcome `voted`, recall that this prediction is based on a range of
likelihoods from 0 to 1. These predictions can be useful to supplement
causal inferences methods like propensity score matching that require
precise likelihood estimates without having to explain why.






APPENDIX: ADVANCED TOPICS

# Random Forests

The last algorithm we'll look at is the random forest, which is an
**ensemble** of decision trees. Ensembling is a common machine learning
approach in which we combine many smaller models into one final
predictions.

A decision tree is a method of stratifying the feature space into a
number of simple regions. Usually, trees use the mean or mode dependent
variable value for the training data in the region to which it belongs
(James *et al.* 2021). Trees can be applied to both regression or
classification problems. To build a single tree, we:

1.  Divide the feature space into distinct regions.
2.  For every observation that falls into a distinct region ($R_j$) we
    make the same prediction.

Trees by themselves turn out to not be very good at prediction compared
to other supervised learning model. However, combine them into a *random
forest* and they become very useful, especially if the relationship is
nonlinear and complex. A random forest (Breiman 2001) is an example of
an ensemble method, which is an approach that combines many simple
models in order to get a single powerful prediction model.

In a random forest, we build a number of decision trees on bootstrapped
training samples. Each tree is grown independently on random samples of
the observations, a process known as *bagging.*

When building the trees, each time a split occurs, we take a random
sample of features to be split candidates instead of the full set of
features. By choosing a random subset each time, a random forest
decorrelates the individual decision trees. This makes the resulting
average less variable.

Let's try fitting a random forest using the `rand_forest` model:

```{r}
# Create model specification 
rf_model <- rand_forest(
  mtry = tune(),
  min_n = tune(),
  trees = 50) %>%
  set_mode("classification") %>%
  set_engine("ranger") 
# Create recipe
rf_recipe <- recipe(union ~ ., data = class_train) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_normalize(all_predictors())
# Create workflow
rf_wflow <- workflow() %>%
  add_recipe(rf_recipe) %>%
  add_model(rf_model)
```

Random forests are known for having many hyperparameters than need
tuning. In this case, we need to tune two: `mtry` and `min_n`. Let's
create the `rsample` object for using 3 folds. Then, we'll tune the
grid. This time, we'll just specify `grid = 10` to indicate we want 10
hyperparameter configurations. The `dials` package, in this case, will
automatically choose those values for us.

```{r}
# Use cross-validation to tune our model appropriately 
trees_folds <- vfold_cv(class_train, v = 3)

# Find appropriate tuning parameters
set.seed(345)
rf_fit <- tune_grid(
  object = rf_wflow, 
  resamples = trees_folds, 
  grid = 10)
```

We're going to use a new metric this time called the ROC-AUC. This is
the Area Under the Curve for the Receiver-Operator Characteristic. The
main takeaway for this metric is that it tells us how good our
probability predictions are, rather than just the predictions. In
classification settings, it's often preferable to the accuracy. It
varies from 0 to 1, where larger is better:

```{r}
# View the ROC metric across models 
rf_fit %>% 
  collect_metrics() %>%
  filter(.metric == "roc_auc") %>%
  select(mean, min_n, mtry)
```

Now, let's finalize the workflow:

```{r}
# Select the best RF model 
best_rf_auc <- select_best(rf_fit, "roc_auc")

final_rf_wflow <- rf_wflow %>%
  finalize_workflow(parameters = best_rf_auc) %>%
  fit(data = class_train)
```

And evaluate the ROC AUC:

```{r}
final_rf_wflow %>%
  augment(new_data = class_test) %>%
  roc_auc(truth = default, estimate = .pred_No)
```

That's rather good ROC performance!

# Overview

Congratulations, you've made it! We covered the basics of supervised
machine learning in `tidymodels` in this workshop. However, there's much
more to explore. The best way to keep pushing forward is to choose a
problem to study, and refer to the documentation when you need help. The
website Kaggle has an abundance of good data science problems to work on
if you need help choosing a task!

# Other classification problems

-   sentiment analysis with natural language processing--positive,
    negative, or neutral problems.
-   
