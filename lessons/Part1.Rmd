---
title: "R Machine Learning Pilot (Day 1)"
output: html_notebook
---

# Learning Objectives

Welcome to R Machine Learning with `tidymodels`. Our goals for this workshop are:

1.  Discuss what machine learning is, and which problems it is most (and least) equipped to address.

2.  Learn about the `tidymodels` framework to implement supervised machine learning models in R, covering pre-processing, regularization, and cross-validation.

3.  Apply the `tidymodels` framework to explore multiple machine learning algorithms in R.

Throughout this workshop series, we will use the following icons:

üîî **Question**: A quick question to help you understand what's going on.

ü•ä **Challenge**: Interactive exercise. We'll go through these in the workshop!

‚ö†Ô∏è **Warning**: Heads-up about tricky stuff or common mistakes.

üí° **Tip**: How to do something a bit more efficiently or effectively.

üìù **Poll**: A zoom poll to help you learn.

üé¨ **Demo**: Showing off something more advanced so you know what you can use R for in the future

# Install Packages

Be sure to have the following packages installed for this workshop:

```{r install, include=FALSE}
# install.packages(c("tidyverse","tidymodels"))
library(tidymodels)
library(tidyverse)

# Prefer tidymodels functions in any case of name conflict
tidymodels::tidymodels_prefer() 

# set seed for reproducibility 
set.seed(123)
```

# What is Machine Learning?

Machine learning refers to the automated detection of meaningful patterns in data (Shalev-Schwartz and Ben-David, 2014). For this reason, some authors often use the term "statistical learning" to reference the same concept (James et al., 2021). The overall goal of machine learning is to develop models that are both dynamic and data-dependent.

## What distinguishes machine learning methods from traditional statistics?

Machine learning and traditional statistics differ in their primary objectives and methodologies. Machine learning focuses on developing predictive models and often uses complex algorithms to find patterns in data, while traditional statistics aims to draw inferences about populations and relationships between variables using hypothesis testing and parametric models. Machine learning often works with large data sets, while traditional statistics may work with smaller, carefully designed samples.

Machine learning places a strong emphasis on predictive accuracy, while traditional statistics emphasizes hypothesis testing and parameter estimation within a framework of statistical significance.

üìù **Poll 1:** Which of these research applications are well suited for machine learning methods? Select all that apply.

1.  Diagnosing a patient's illness based on a cluster of symptoms and other health data.

2.  Explaining why some states have higher voter participation rates than others.

3.  Estimating the impact of unionization on worker pay.

4.  Identifying businesses to audit for tax evasion.

5.  Predicting commuting times for workers in urban areas.

**Solutions**: 1, 4, 5

# Example of a Machine Learning Problem

The core goal of machine learning is to develop models that can generalize patterns from data that can perform tasks or make predictions on new, unseen data. We will often have some output variable (denoted $y$) that we want to predict based on a set of input variables (denoted $\mathbf{X}$).

Mathematically, we suppose that there is a relationship between the output variable and the features, which we can write in a general model form as:

$Y = f(\textbf{X})+ \epsilon$

where $f$ is a fixed but unknown function of our feature vector $\textbf{X} = (X_1, X_2,...,X_n)$ and $\epsilon$ is a random error term that is independent of the $\mathbf{X}$ vector. In general, the function that connects the target variable and the features is unknown, so the algorithms we use are ways to *estimate* the relationship.

**Example 1: Employment Survey Data**

To ground symbols in code, let's explore our primary data set for the workshop: the Current Population Survey (CPS).

First, we'll load the data:

```{r}
jobs <- read.csv("../data/jobs.csv",row.names = NULL)

# visually inspect the data frame 
view(jobs)

```

## ü•ä Challenge 1: Predicting Wage from Age

Let's try to predict an individual's hourly wage based on their age alone using linear regression. Linear regression is a widely used statistical method that models the relationship between a continuous output variable and one or more input variables by fitting a linear equation to observed data. In the univariate case (i.e., y and a single x), linear regression looks like this:

![Linear Regression](../images/linearregression.png)

We can plot variables to see the relationship with `ggplot2` and add a best fit line. Let's first examine the relationship between a worker's age (`age`) and their hourly wage (`hourly_wage`).

```{r}
jobs %>%
  ggplot(aes(x = age, y = hourly_wage)) +
  geom_point(color = "blue") +
  geom_smooth(method = "lm", color = "red", se = F) + 
  labs(title = "Scatter Plot of Age vs. Hourly Wage with Best Fit Line", 
       x = "Age", 
       y = "Hourly Wage") + 
  theme_minimal() 
```

While it may not seem obvious, we have just implemented our first machine learning algorithm! Specifically, we have tried to predict the relationship between a single feature (`age`) and a target variable (`hourly_wage`) using linear regression. Notice that the focus is on predicting wages, rather than the value of the age parameter.

Here, the learning function $f$ can be written as:

$\texttt{hourly_wage} = f(\texttt{age}) + \epsilon$

üîî **Question 1**: Notice that we have called *y* the "outcome" or "target" variable, and the set of $\mathbf{X}$ variables "features" or "inputs." How does this language reflect a conceptual shift from traditional statistics, where we typically use language like "independent" and "dependent" vaariables?

**Solution**: We are focused on predicting the value of y, and are moving away from traditional statistics by not making assumptions about the nature of the causal relationship between our x and our y.

# Refining Our Research Question

Now, let's start building a more complex model that utilizes the other features we have available that are likely related to one's wage, including education level, race, sex, and occupation.

## ü•ä Challenge 2: Exploratory Analyses

In Challenge 1, we looked at the relationship between a continuous x feature, `age`, and our target, `hourly_wage`. Perform your own exploratory analyses, but this time examine at least one categorical feature (or features) that you think might be useful in predicting `hourly_wage`. In the companion resource guide, we provide a comprehensive data dictionary.

```{r}
# Histogram of Wage by Sex
jobs %>%
  ggplot(aes(x = hourly_wage, fill = worker_status)) +
  geom_histogram(position = "identity", alpha = 0.5, bins = 30) +
  labs(title = "Overlapping Histogram of Hourly Wage by Time Contract",
       x = "Wage",
       y = "Frequency",
       fill = "Full- or Part-Time Status") +
  theme_minimal()
```

# The Machine Learning Pipeline

Now that we have a better sense of our data's structure, let's start building out our machine learning algorithm to predict an individual's hourly wage!

To do so, we will draw primarily on `tidymodels`, an ecosystem of R packages designed to provide a consistent and tidy interface for modeling and machine learning tasks. It is built on top of the principles of the `tidyverse` and shares a consistent design philosophy, making it easy to integrate with other `tidyverse` packages like `dplyr` and `ggplot2`.

[KQ NOTE: LOOKING FOR GOOD DIAGRAM OF ML PIPELINE, HAVING TROUBLE FINDING A GOOD ONE FOR SUPERVISED ML, WILL MAKE OWN--\> SET EXPECTATIONS FOR WHAT WE'LL COVER IN PART 1 AND 2]

We will follow these steps:

1.  Pre-process our data so it is in the appropriate format.

2.  Split data into training and test sets.

3.  Use hyperparameter tuning on our training data to select the best configuration for our model.

4.  Train the machine learning algorithm.

5.  Evaluate the performance of the final model on the test set.

6.  Refine the models.

7.  Operationalize algorithm on new data.

# Step 1: Pre-Processing

Sometimes, our features are not in the best format for our model to use effectively. Data is messy, and often needs to be transformed in order for a machine learning model to be able to be fit.

## Data Types

Let's take a look at a few variables in our data set `jobs`.

```{r}
jobs %>% 
  select(hours_weekly, union, state) %>% 
  slice_sample(n=10)
```

Looking at the first 10 rows, we see that `hours_weekly` is a numeric variable; `union` is a categorical variable with two values; and `state_fip` is categorical variable that is coded as numeric. If we were to include these variables in a linear regression model as is, we would run into a few issues.

#### Categorical Variables

Let's first look at `state`, which is a numeric variable that corresponds to a state's FIPS code, the census ID for the various state and territories in the United States. For value labels, refer to the data dictionary in the companion guide.

```{r}
jobs %>%
  ggplot(aes(x = state, y = hourly_wage)) +
  geom_point(color = "blue") +
  geom_smooth(method = "lm", color = "red", se = F) + 
  labs(title = "Scatter Plot of State Code vs. Hourly Wage with Best Fit Line", 
       x = "Numeric State Code", 
       y = "Hourly Wage") + 
  theme_minimal() 
```

Here, we run into the issue that each state's value corresponds to a number when it is actually a categorical variable. We will need to recode this and other categorical variables for the modeling process.

#### Missing Values

We also see that `hours_weekly` has missing values, which is a common problem in real world data. Depending on the use, in your own work you may choose to drop data with missing values, or impute missing values based on certain criteria. In this example, we will drop rows with missing values, but in a later example, we will add imputation to our pre-processing. 

## ü•ä Challenge 3: Assess Data Quality

Determine the extent to which missing values are a problem in this data set. 
```{r}
colSums((is.na(jobs)))
```

Now that we have a better sense of our data and the types of variables we have, let's proceed. 

## Recipes

The process of preparing our features (i.e., variables) to improve the modeling process is called **feature engineering**. Within the `tidymodels` framework, the functions to implement feature engineering are housed in the `recipes` package (note: this package is embedded in `tidymodels` so we do not need to load the package).

A recipe is an object that defines the series of steps needed for any data processing for the model. Unlike the formula method inside a modeling function, the recipe defines the steps without immediately executing them; it is only a specification of what should be done.

Let's start by taking a look at a recipe used to an individual's hourly wage from other features:

```{r}
jobs <- jobs %>% select(-c(paid_hour,class_worker))

wage_recipe <- 
  recipe(hourly_wage ~ ., data = jobs) %>%
  step_naomit(all_predictors()) %>%
  step_dummy(all_nominal_predictors())
wage_recipe
```

This recipe provides the model that we eventually want to run and a step to convert nominal features into dummy variables. Let's break this down one component at a time:

-   The call to `recipe()` with a formula tells R the roles of the variables. Here, our dependent variable is `hourly_wage`.

-   What follows the `recipe` call is a series of `steps`. The first one is a step called `step_naomit()`, which omits any samples with missing data. Each step function accepts the columns that it operates on. In this case, we apply `step_naomit` to every column, using the `all_predictors()` function. While this example does not contain missing values, a later example will

-   Next, `step_dummy()` is used to specify which variables should be converted from a qualitative format to a quantitative format, in this case, using dummy or indicator variables.

-   The function `all_nominal_predictors()` captures the names of any predictor columns that are currently factor or character.

`tidymodels` has a variety of data pre-processing step functions [available](https://recipes.tidymodels.org/reference/index.html). While we will not cover how to do so, it is also possible to write custom pre-processing functions.

## Applying a `recipe` to Data

Thus far, we have created a recipe, but we have not applied it to data. To do so, we need to use two
functions: `prep()` and `bake()`.

First, we `prep()` any recipe with at least one pre-processing operation. `prep()` applies the recipe to the training data, and returns an updated recipe with the appropriate estimates. We can think of prep in the baking analogy as getting all your ingredients ready to go on the counter. After we `prep()` the recipe, we have to `bake()` the recipe by taking the pre-processing steps and applying them to a data set.

```{r}
prepped_recipe <- prep(wage_recipe)
prepped_recipe
```

Now, let's bake:

```{r}
baked_jobs <- bake(prepped_recipe, new_data = jobs_train)
baked_jobs 
```

Take a look at the columns in the `baked_jobs` data. What has changed?

# Step 2: Train-Test Split

## The bias-variance trade-off

Before we proceed, let's discuss a fundamental concept in machine learning: the bias-variance trade-off. The bias-variance trade-off refers to the tension associated with model simplicity and flexibility, and describes the relationship between the two types of errors a model can make: bias and variance.

-   Bias: the difference between a model's prediction and the actual value of an observation
-   Variance: the complexity of the model

![Over and Underfitting](../images/overfit.png)

In the initial example, we attempted to strike this balance by using linear regression to predict an individual's wage based on their age alone. We could have used $y = f(x)$, but this would have overfit our model by introducing high variance, and thus performed poorly on new, unseen data. Alternatively, we could have used $y = \bar{x}$, but this would have underfit our model due to its high bias.

It is essential to understand this trade-off because it helps in building models that generalize well and make accurate predictions.

## The train-test split

Performing a train-test split is an approach to managing the bias-variance trade-off. It refers to the division of a data set into two subsets: one for training the model and another for evaluating its performance. The training set is used to teach the model, while the test set serves as an independent data set to assess how well the model generalizes to new, unseen data. Once we have partitioned our data into the training and test sets, we will not touch the test set until we are ready to evaluate our model.

üîî **Question 2**: How does splitting our data into the training and test sets address the bias-variance trade-off described earlier? 

**Answer**: This split is crucial for preventing overfitting, providing an unbiased evaluation of the model's performance, and ensuring that the model can make accurate predictions on data it has not encountered during training.

## Performing the split

```{r}
# Perform train/test split
jobs_split <- jobs %>% initial_split(prop = 0.80)
```

The resulting object is a `rsplit` object which contains the partitioning information for the data. To get the training and test data, we apply two additional functions. The resulting data sets have the same columns as the original data but only the appropriately sampled rows.

```{r}
jobs_train <- training(jobs_split)
jobs_test <- testing(jobs_split)
print(dim(jobs_train))
print(dim(jobs_test))
```

# Training the Model 

Within the `tidymodels` package, the `parsnip` package provides a fluent and standardized interface for various models (Kuhn and Silge, 2021). This modeling approach follows the design paradigm of the package.

There are a variety of models you can fit using `parsnip` (take a look at the documentation
[here](https://parsnip.tidymodels.org/reference/index.html)). We're going to use the `linear_reg` function from `parsnip` to create a linear regression model. Check out the documentation
[here](https://parsnip.tidymodels.org/reference/linear_reg.html).

You'll notice that `linear_reg` has several input arguments we can specify, including `engine` and `mode`. These two arguments respectively indicate what software package will be used for learning the parameters, and what type of problem we're solving (in this case, a regression).

For now, we should be fine using the default arguments. So, we start by creating a model.

```{r}
model <- linear_reg()
model
```

Next, we fit the model using an R formula:

```{r fit_model}
fitted_model <- model %>% 
  fit(hourly_wage ~ age + race + education + sex + hispanic + marital_status + worker_status + union + occupation + industry,
      data = jobs_train)
fitted_model
```

## Evaluating the Model

Now we have a model. It's a basic model, and the next reasonable question is how well it works to solve the problem of estimating the relationship. We prefer to have a quantitative approach to estimate effectiveness to compare different models or tweak our model to improve performance. In `tidymodels` this approach is empirically data-driven. That means that we use the test data to measure the model's effectiveness.

It is important to note that we keep the training and test data set apart. We can run any tweaks that we want to our training set, but we should leave the test set alone until we are ready to evaluate our models. Methods for evaluation within the `tidymodels` universe are from the `yardstick` package.

The general syntax for a metrics function in `tidymodels` is as follows:

```{r, eval = F}
function(data, truth, ...)

```

where the data argument is a data frame or tibble, and the truth argument is the column with observed outcome values. Additional arguments (...) can be used to specify columns containing features.

To use this function, we need predictions from the model. The `predict()` method can be used to obtain a tibble with the predictions from our model on new data. We can match these values with the
corresponding observed outcome values.

```{r predictions}
results <- predict(fitted_model, new_data = jobs_test) %>%
  bind_cols(jobs_test %>% select(hourly_wage))
results
```

```{r plot_predictions}
# We can plot the data prior to computing metrics for a visual inspection of fit
results %>%
  ggplot(aes(x = hourly_wage, y = .pred)) +
  geom_abline(lty = 2) +
  geom_point(alpha = 0.7) +
  labs(x = "Hourly Wage", y = "Predicted Values") +
  coord_obs_pred()
```

Now, we can perform the evaluation. The full suite of metrics functions
is available
[here](https://yardstick.tidymodels.org/reference/index.html). Since our
running example is a linear regression, let's start with $R^2$, since
that's among the easiest metrics to interpret:

```{r}
rsq_trad(results, truth = hourly_wage, estimate = .pred)
```

We can do multiple metrics at once, though. Let's pick three common
metrics for linear regression models - $R^2$, Root Mean Square Error
(RMSE), and Mean Absolute Error (MAE) - and evaluate them at the same
time using `yardstick`.

```{r evaluate_metrics}
# Make a set of metrics
jobs_metrics <- metric_set(rmse, rsq, mae)
jobs_metrics(results, truth = hourly_wage, estimate = .pred)
```

## Interpreting the Model

We saw above that we could examine the coefficients of the fitted model.
We can view a tidy version of these results to better analyze them:

```{r}
tidy(fitted_model)
```

The nice thing about this function is that it provides the estimates, standard errors, p-values, etc.

An important aspect of machine learning is being able to interpret what our model is telling us. What can we say about how each of the features corresponds with the output, according to the linear model?

For example, the bill depth coefficient tells us that bill length increases by roughly 0.57 mm for every mm increase of bill depth.

------------------------------------------------------------------------

Congratulations, you've trained your first machine learning model in `tidymodels`! We'll now start to further develop our pipelines.



    

















## ü•ä Challenge 4: Stratifying a Split

In the above example, we performed the split randomly across samples. However, we might not always want to do this. For example, one's income/wage often corresponds to their level of education

1.  Discuss: What happens if, by chance, all the samples corresponding
    to species A end up in the training set, and all the samples
    corresponding to species B end up in the test set? Will this impact
    performance?

What we'd like to do is ensure that, for some variable (e.g., species),
equal proportions of samples for each value appear in the training set
and test set. This ensures that species A samples are present in both
the training and test sets. This process is called **stratifying**.

2.  Use the `strata` argument in `initial_split` to stratify according
    to `education`. Perform the split to obtain training and test sets.

```{r}
jobs_train <- training(jobs_split)
jobs_test <- testing(jobs_split)
print(dim(jobs_train))
print(dim(jobs_test))

table(jobs$education)
```
