---
title: "R Machine Learning Pilot Solutions (Day 2)"
output: html_notebook
editor_options: 
  markdown: 
    wrap: 72
---

## 📝 Poll 1: Classification vs. Regression

Identify which of the following are classification problems in machine
learning.

1.  An advertiser is interested in the relationship between age and the
    number of hours of YouTube consumed.

2.  A medical testing company conducts a procedure to determine whether
    a person has a cancer diagnosis.

3.  A researcher is interested in the effect of an education
    intervention on students' test scores.

4.  A software engineer is designing an algorithm to detect whether an
    email is spam or not.

5.  A political scientist wants to classify Twitter posts as positive or
    negative.

**Solution 1: (2), (4), and (5).**

## 🥊 Challenge 1: Exploratory Data Analysis

Perform exploratory analyses on the `vote2020` data set, keeping in mind
that today's goal is to predict `voted`. What do you notice about the
data?

```{r}
# create bar chart of voting percentages by state 
vote2020 %>%
  group_by(state) %>%
  summarise(voted_percentage = mean(voted, na.rm = TRUE) * 100) %>%
  ungroup() %>%
  ggplot(aes(x = reorder(state,-voted_percentage), y = voted_percentage)) +
    geom_bar(stat = "identity", fill = "blue") +
    theme(axis.text.x = element_text(angle = 45, vjust = 0.5, hjust = 1)) +  # Adjusting x-axis text +
  coord_flip() + 
  scale_y_continuous(limits=c(0,100)) + 
    labs(title = "Percentage of People Who Voted in Each State",
         x = "State",
         y = "Percentage Voted") +
    theme_minimal()
```

## 🔔 Question 1: Train-Test Split

A researcher used the entire `vote2020` data set to train a model that
resulted in impressively high accuracy during evaluation. However, upon
deploying this model to predict voter turnout for an upcoming election,
the predictions significantly diverged from actual turnout, with many
discrepancies in who was predicted to vote versus who actually voted.
The model's near-perfect performance in development starkly contrasted
its poor real-world prediction outcomes, indicating a potential
oversight in the model training and evaluation process. Why might that
be?

**Answer 1**: The key issue is that the model was never tested on unseen
data. By using the entire data set for training, the model essentially
"memorized" the data, including any noise or patterns specific to that
set of individuals. This led to overfitting, where the model excelled at
predicting the training data but failed to generalize to new, unseen
data. Since the model's performance was only evaluated on data it had
already seen, its apparent accuracy was misleading, giving a false sense
of confidence in its predictive capabilities.

## 🥊 Challenge 2: Creating a Recipe

Using the same logic from Part 1, create your own recipe called
`voterecipe` that lays out the steps for pre-processing the data.
Instead of dropping rows with missing values, use
[step_impute_median](https://recipes.tidymodels.org/reference/step_impute_median.html)
to impute numeric variables and
[step_impute_mode](https://recipes.tidymodels.org/reference/step_impute_mode.html)
to impute categorical variables. *Hint:* you can select all numeric
variables with `all_numeric_predictors()`.

```{r}
voterecipe <- 
  recipe(voted ~ ., data = vote_train) %>%
  step_impute_median(all_numeric_predictors()) %>%
  step_impute_mode(all_nominal_predictors()) %>%
  step_dummy(all_nominal_predictors()) 
```

## 🔔 Question 2: Imputation

Notice that we have only applied our recipe to the training data so far.
Explain why it is important to perform imputation separately on the
training and test sets rather than imputing missing values before
splitting the data set. Consider why we don't want the data we train our
model on to be exposed to our test data set in the first place.

**Answer 2**: We want to keep our training and test sets separate. If we
impute missing values in the entire data set using the mean, the mean
calculation will include information from both the training and test
sets. This means the model gets indirectly exposed to information from
the test set during training, which can lead to overly optimistic
performance estimates and a model that may not perform as well on truly
unseen data. This concept is referred to as 'data leakage.'

## 🔔 Question 3: Analyzing Output

Notice that four new columns have appeared in our data set. What do
these values mean? What are they telling us?

**Answer 3**: `.pred_class` corresponds to the value our algorithm is
predicting, i.e., whether the person voted in the election or not. This
was determined using the two variables `.pred_0` and `.pred_1`, which
represent the predictions from the logistic regression. If membership in
group 1 (i.e., voted = 1) is greater than .5, then `.pred_class` will
take a value of 1.

## 🥊 Challenge 3: Changing Hyperparameters

We have reproduced the original code from above that trained a basic
classifier on our voting data set without changing any of the
hyperparameters, as well as the code that obtains predictions. Re-run
this code several times, but each time change the hyperparameters in the
model specification. For penalty, include a non-negative number; and for
engine, select either "glmnet" or "glm". How does the accuracy change?

```{r}
chal3_model <- logistic_reg(mode = "classification", 
                               engine="glmnet", 
                               penalty=0)

chal3_wflow <- workflow() %>%
  add_recipe(voterecipe) %>%
  add_model(chal3_model)

chal3_fit <- fit(chal3_wflow, vote_train)

chal3_predictions <- augment(chal3_fit, new_data = vote_test)

accuracy(chal3_predictions, truth = voted, estimate = .pred_class, type="class")
```

## 🔔 Question 4: Analyzing a Grid

We have a hyperparameter grid with 10 rows. What does each of these
signify?

**Solution 4**: Each is a potential set of hyperparameters, or
arguments, for our logistic regression. R will try each combination to
find the one that generates the most accurate prediction. As we add more
hyperparameters, the combinations will increase exponentially.

## 🔔 Question 5: Accuracy on the Test Set

Notice our final accuracy on the test set is lower than our accuracy on
the training set. Why might that be?

**Solution 5**: We selected our hyperparameters (i.e., the penalty
function) to yield the highest output *specifically* on our training
set.
